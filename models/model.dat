# data means
0.5012546181678772 0.49904489517211914 0.00031128962291404605 0.001587016275152564 0.0050786384381353855 0.5009795427322388

# data standard deviation
0.2881629168987274 0.2879659831523895 0.70692378282547 0.7072914242744446 0.010740356519818306 0.2882631719112396

# Model Architecture
Activation_function = ReLU
Number_of_layers = 3

# Layer: layers.0.weight
10 5
0.593612313271 0.040711343288 0.183820992708 0.752557456493 0.001837853226 
0.189125910401 -0.309185922146 -0.410034060478 0.758466124535 -0.000418064505 
0.148224771023 0.486652135849 0.086343482137 -0.625592887402 -0.001242849859 
0.419771164656 -0.411963224411 0.203426554799 -0.459097862244 0.002616644138 
0.389785081148 -0.068655490875 -0.714358270168 -0.036467168480 0.000652246119 
-0.363304167986 -0.221350997686 0.380619078875 0.682384788990 -0.001385899959 
-0.321470111609 -0.151105850935 0.266825765371 -0.417257487774 -0.000404539314 
0.101204603910 0.317216515541 0.829514980316 0.322068840265 0.002386739478 
0.129345387220 0.324679881334 -0.235704869032 0.487870663404 0.000153531713 
-0.451951950788 -0.185939371586 -0.605054140091 0.263633877039 0.001202955144 

# Layer: layers.0.bias
10
0.614146530628 -0.124045588076 0.074057623744 0.097913421690 0.411487877369 0.240854755044 0.273837029934 0.680259048939 -0.056686405092 0.006794080604 

# Layer: layers.1.weight
10 10
-0.054495323449 0.229520365596 0.521151006222 -0.042313639075 0.013901988976 0.487626850605 0.122784063220 -0.091958373785 -0.271602392197 -0.305348902941 
0.280180186033 0.901218473911 -0.356632053852 -0.138738587499 -0.107829019427 0.298495501280 0.504199028015 0.203396901488 -0.662029266357 -0.424892842770 
0.386812448502 -0.144987910986 -0.661545813084 -0.219872236252 0.907492041588 -0.148960307240 -0.581301808357 -0.601232469082 -0.059076733887 -0.002686012536 
0.308806985617 0.148034825921 -0.116895139217 -0.059256963432 -0.110264614224 0.174530893564 -0.298413544893 -1.055018305779 -0.190603375435 -0.342655956745 
0.224310308695 -0.071941196918 -0.087403446436 -0.379676729441 -0.006840188988 -0.073889769614 -0.422102332115 0.557793378830 0.665693581104 0.433011740446 
0.025012640283 0.061840169132 0.388915508986 -0.334178894758 -0.815280199051 0.486185938120 0.128505736589 0.293601602316 0.325696825981 0.262935727835 
0.467790365219 0.027045119554 0.627960681915 -0.107098542154 0.425663232803 0.251371502876 -0.648880064487 -0.619631469250 0.226680040359 0.052949536592 
-0.895712673664 -0.277820259333 0.606471776962 -0.145953238010 0.117203749716 -0.402248769999 0.271556943655 0.266233474016 -0.128169506788 -0.095031723380 
-0.175829470158 -0.394974470139 -0.059841059148 0.372040510178 0.055932041258 0.050348415971 0.199667170644 -0.409676879644 0.196898564696 0.452688366175 
0.315529286861 0.151920750737 -0.241424828768 0.374772936106 -0.406639784575 -0.252395719290 -0.067450918257 0.128105074167 0.457699537277 0.228252127767 

# Layer: layers.1.bias
10
-0.121549606323 -0.157570302486 -0.065175600350 -0.116697728634 0.278311431408 -0.182424604893 -0.162027850747 -0.152138337493 0.302013814449 0.317654252052 

# Layer: output_layer.weight
1 10
0.971934795380 0.735329389572 0.658175170422 -1.214447617531 -0.492457956076 0.652174472809 0.831557035446 0.606347978115 -0.803372025490 -0.912776291370 

# Layer: output_layer.bias
1
-0.299526304007 
